{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8db171-dd8b-49a4-8ca7-4275ecb1cd58",
   "metadata": {},
   "source": [
    "## CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae83587c-11b2-4e0b-a60a-3b7b679374cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting optuna\n",
      "  Using cached optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Using cached alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.35)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached optuna-4.0.0-py3-none-any.whl (362 kB)\n",
      "Using cached alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "Using cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Using cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: nltk, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 alembic-1.13.3 colorlog-6.8.2 nltk-3.9.1 optuna-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9955a1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 22:19:53.641217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-26 22:19:57.941210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {},
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea97c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define stop_words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e78b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # convert text to lowercase\n",
    "    text = re.sub(r'\\b(u\\.s\\.|us)\\b', 'usa', text, flags=re.IGNORECASE)  # replace \"U.S.\" or \"US\" with \"usa\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'&\\w+;', '', text)  # remove HTML entities\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove numbers and punctuation (keep only letters and spaces)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] # remove stop-words, tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatization\n",
    "    return ' '.join(words) # combining words into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f875b9-c44e-45c2-9413-75e7e56a7cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_df = pd.read_csv('../data/agn_train.csv')\n",
    "test_df = pd.read_csv('../data/agn_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1b069c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.7 s, sys: 420 ms, total: 47.1 s\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Applying preprocessing to titles and descriptions\n",
    "train_df['clean_text'] = (train_df['Title'] + ' ' + train_df['Description']).apply(preprocess_text)\n",
    "test_df['clean_text'] = (test_df['Title'] + ' ' + test_df['Description']).apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)  # Limit the number of words\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])  # Tokenize based on the training data\n",
    "\n",
    "# Tokenizing titles and descriptions\n",
    "X_train_title_seq = tokenizer.texts_to_sequences(train_df['Title'])\n",
    "X_test_title_seq = tokenizer.texts_to_sequences(test_df['Title'])\n",
    "\n",
    "X_train_description_seq = tokenizer.texts_to_sequences(train_df['Description'])\n",
    "X_test_description_seq = tokenizer.texts_to_sequences(test_df['Description'])\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length_titles = max([len(x) for x in X_train_title_seq])\n",
    "max_length_descriptions = max([len(x) for x in X_train_description_seq])\n",
    "\n",
    "# Padding sequences to make them of equal length\n",
    "X_train_title_pad = pad_sequences(X_train_title_seq, maxlen=max_length_titles)\n",
    "X_test_title_pad = pad_sequences(X_test_title_seq, maxlen=max_length_titles)\n",
    "\n",
    "X_train_description_pad = pad_sequences(X_train_description_seq, maxlen=max_length_descriptions)\n",
    "X_test_description_pad = pad_sequences(X_test_description_seq, maxlen=max_length_descriptions)\n",
    "\n",
    "# Assign labels for the training data\n",
    "y_train = train_df['Class Index'].values - 1  # Adjust class indices to [0, 1, 2, 3]\n",
    "\n",
    "# Assign labels for the test data\n",
    "y_test = test_df['Class Index'].values - 1  # Adjust class indices to [0, 1, 2, 3]\n",
    "\n",
    "# Шаг 1: Скачивание и распаковка GloVe эмбеддингов\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "\n",
    "# Loading pre-trained GloVe embeddings\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Creating an embedding matrix for the words in the tokenizer\n",
    "embedding_matrix = np.zeros((5000, 100))  # input_dim=5000, output_dim=100 (GloVe vector size)\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 5000:  # Only use the top 5000 words\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Using pre-trained GloVe embeddings\n",
    "embedding_layer = Embedding(input_dim=5000, \n",
    "                            output_dim=100, \n",
    "                            weights=[embedding_matrix],  # Passing the pre-trained embedding matrix\n",
    "                            trainable=False)  # Embeddings are not trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {},
   "source": [
    "### Training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 22:20:54.634858: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:20:54.637936: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:20:54.640069: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-10-26 22:20:54.960173: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:20:54.962951: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:20:54.964959: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 22:20:55.551193: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:20:55.554267: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:20:55.556758: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-10-26 22:20:55.848718: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:20:55.851989: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:20:55.854382: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-10-26 22:20:57.730414: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:20:57.733365: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:20:57.735536: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-10-26 22:20:58.019673: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:20:58.023049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:20:58.025624: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3375/3375 [==============================] - 385s 114ms/step - loss: 0.2621 - accuracy: 0.9087 - val_loss: 0.2918 - val_accuracy: 0.8923\n",
      "Epoch 3/10\n",
      " 859/3375 [======>.......................] - ETA: 4:47 - loss: 0.1988 - accuracy: 0.9311"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Input for titles\n",
    "title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "title_embedding = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)(title_input)\n",
    "\n",
    "# Добавление свёрточного слоя CNN\n",
    "title_conv = Conv1D(filters=128, kernel_size=5, activation='relu')(title_embedding)\n",
    "title_pooling = MaxPooling1D(pool_size=2)(title_conv)\n",
    "\n",
    "# Добавление слоя LSTM\n",
    "title_lstm = LSTM(128, return_sequences=False)(title_pooling)\n",
    "\n",
    "# Input for descriptions\n",
    "description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "description_embedding = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)(description_input)\n",
    "\n",
    "# Добавление свёрточного слоя CNN\n",
    "description_conv = Conv1D(filters=128, kernel_size=5, activation='relu')(description_embedding)\n",
    "description_pooling = MaxPooling1D(pool_size=2)(description_conv)\n",
    "\n",
    "# Добавление слоя LSTM\n",
    "description_lstm = LSTM(128, return_sequences=False)(description_pooling)\n",
    "\n",
    "# Merging the title and description representations\n",
    "merged = Concatenate()([title_lstm, description_lstm])\n",
    "\n",
    "# Fully connected layers after merging\n",
    "dense_1 = Dense(128, activation='relu')(merged)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)  # 4 classes\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training the model with early stopping\n",
    "model.fit([X_train_title_pad, X_train_description_pad], \n",
    "          y_train, \n",
    "          validation_split=0.1, epochs=10, batch_size=32, verbose=1, \n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cf88e",
   "metadata": {},
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 22:56:45.168724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:56:45.173755: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:56:45.176482: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-10-26 22:56:45.550695: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-10-26 22:56:45.555959: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-10-26 22:56:45.559163: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Defining hyperparameters with Optuna\n",
    "def model_builder(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 64, 256)  # Number of LSTM units\n",
    "    cnn_filters = trial.suggest_int('cnn_filters', 64, 256)  # Number of CNN filters\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 5)  # Kernel size for CNN\n",
    "    pool_size = trial.suggest_int('pool_size', 2, 3)  # Pool size for MaxPooling\n",
    "    num_units = trial.suggest_int('num_units', 64, 512)  # For dense layers\n",
    "\n",
    "    # Input for titles\n",
    "    title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "    title_embedding = Embedding(input_dim=5000, output_dim=100, \n",
    "                                weights=[embedding_matrix], trainable=False)(title_input)\n",
    "    \n",
    "    # CNN + LSTM for titles\n",
    "    title_conv = Conv1D(filters=cnn_filters, kernel_size=kernel_size, activation='relu')(title_embedding)\n",
    "    title_pooling = MaxPooling1D(pool_size=pool_size)(title_conv)\n",
    "    title_lstm = LSTM(lstm_units, return_sequences=False)(title_pooling)  # LSTM for title\n",
    "    \n",
    "    # Input for descriptions\n",
    "    description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "    description_embedding = Embedding(input_dim=5000, output_dim=100, \n",
    "                                      weights=[embedding_matrix], trainable=False)(description_input)\n",
    "    \n",
    "    # CNN + LSTM for descriptions\n",
    "    description_conv = Conv1D(filters=cnn_filters, kernel_size=kernel_size, activation='relu')(description_embedding)\n",
    "    description_pooling = MaxPooling1D(pool_size=pool_size)(description_conv)\n",
    "    description_lstm = LSTM(lstm_units, return_sequences=False)(description_pooling)  # LSTM for description\n",
    "\n",
    "    # Merging the title and description representations\n",
    "    merged = Concatenate()([title_lstm, description_lstm])\n",
    "    \n",
    "    # Fully connected layers\n",
    "    dense_1 = Dense(num_units, activation='relu')(merged)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)  # 4 classes\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for model evaluation\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "\n",
    "    # Train the model with the optimized hyperparameters\n",
    "    model.fit([X_train_title_pad, X_train_description_pad], y_train, \n",
    "              validation_split=0.1, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model's accuracy\n",
    "    loss, accuracy = model.evaluate([X_test_title_pad, X_test_description_pad], y_test, verbose=1)\n",
    "    return accuracy  # Optimizing for accuracy\n",
    "\n",
    "# Running Optuna to search for the best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')  # Maximize accuracy\n",
    "study.optimize(objective, n_trials=10)  # Number of optimization trials\n",
    "\n",
    "# Print the best trial results\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0",
   "metadata": {},
   "source": [
    "### Testing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the best model using the best hyperparameters found by Optuna\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "model = model_builder(best_trial)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, \n",
    "          validation_split=0.1, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert predicted probabilities to predicted classes\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Generate classification report (Precision, Recall, F1-Score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067f72-a519-4449-ac64-8ae3e97291ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
