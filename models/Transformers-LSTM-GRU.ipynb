{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4571f8-1f4c-40f4-93ee-b6800f977d69",
   "metadata": {},
   "source": [
    "## Transformers-LSTM-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734eb92-1388-4c79-ba6f-60c375612e61",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac00fcf-1633-490e-8cfc-d6bab63ae471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, Conv1D, MaxPooling1D, LSTM, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce20941-4a39-4af1-923d-342a88588108",
   "metadata": {},
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa93215-1b22-4871-ba0e-b5764c42d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_url, header=None)\n",
    "test_df = pd.read_csv(test_url, header=None)\n",
    "\n",
    "train_df.columns = ['Class Index', 'Title', 'Description']\n",
    "test_df.columns = ['Class Index', 'Title', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf810f2-c8ba-48f4-91bf-0ec458af9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'\\b(u\\.s\\.|us)\\b', 'usa', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    words = [lemmatizer.lemmatize(word) for word in words] \n",
    "    return ' '.join(words)\n",
    "    \n",
    "# Combine 'Title' and 'Description' into 'clean_text' and preprocess\n",
    "train_df['clean_text'] = (train_df['Title'] + ' ' + train_df['Description']).apply(preprocess_text)\n",
    "test_df['clean_text'] = (test_df['Title'] + ' ' + test_df['Description']).apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_title_seq = tokenizer.texts_to_sequences(train_df['Title'])\n",
    "X_test_title_seq = tokenizer.texts_to_sequences(test_df['Title'])\n",
    "\n",
    "X_train_description_seq = tokenizer.texts_to_sequences(train_df['Description'])\n",
    "X_test_description_seq = tokenizer.texts_to_sequences(test_df['Description'])\n",
    "\n",
    "# Determine max sequence length and pad sequences\n",
    "max_length_titles = max([len(x) for x in X_train_title_seq])\n",
    "max_length_descriptions = max([len(x) for x in X_train_description_seq])\n",
    "\n",
    "X_train_title_pad = pad_sequences(X_train_title_seq, maxlen=max_length_titles)\n",
    "X_test_title_pad = pad_sequences(X_test_title_seq, maxlen=max_length_titles)\n",
    "\n",
    "X_train_description_pad = pad_sequences(X_train_description_seq, maxlen=max_length_descriptions)\n",
    "X_test_description_pad = pad_sequences(X_test_description_seq, maxlen=max_length_descriptions)\n",
    "\n",
    "y_train = train_df['Class Index'].values - 1\n",
    "y_test = test_df['Class Index'].values - 1\n",
    "\n",
    "# Download and load GloVe embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((5000, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 5000:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a547c2-a40b-4a6c-b2bb-93ecb9a2231a",
   "metadata": {},
   "source": [
    "### Basic training module (Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec35f0a-c007-4777-a645-e1792afb7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs and embedding layers\n",
    "title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "title_embedding = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)(title_input)\n",
    "title_gru = GRU(128, return_sequences=True)(title_embedding)\n",
    "title_lstm = LSTM(128, return_sequences=False)(title_gru)\n",
    "\n",
    "description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "description_embedding = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)(description_input)\n",
    "description_gru = GRU(128, return_sequences=True)(description_embedding)\n",
    "description_lstm = LSTM(128, return_sequences=False)(description_gru)\n",
    "\n",
    "merged = Concatenate()([title_lstm, description_lstm])\n",
    "\n",
    "# Fully connected layers after merging\n",
    "dense_1 = Dense(128, activation='relu')(merged)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)\n",
    "\n",
    "# Build and compile model\n",
    "model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training with early stopping\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, \n",
    "          validation_split=0.1, epochs=5, batch_size=16, verbose=1, \n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c783e39-4b83-4176-b057-c77360fc72f3",
   "metadata": {},
   "source": [
    "### Testing module 1 (Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb6ad9-0039-4c5e-a3bb-181214e587af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and metrics for base model\n",
    "y_pred_base = model.predict([X_test_title_pad, X_test_description_pad]).argmax(axis=1)\n",
    "print(\"Base Model Accuracy:\", np.mean(y_pred_base == y_test))\n",
    "print(classification_report(y_test, y_pred_base))\n",
    "conf_matrix_base = confusion_matrix(y_test, y_pred_base)\n",
    "\n",
    "# Display confusion matrix for base model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_base, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Base Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c52bd2-74a2-4bc7-aac0-a506a05ae243",
   "metadata": {},
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9d87e-bf9c-4a83-a676-5db462fadf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(trial):\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    gru_units = trial.suggest_int('gru_units', 64, 256)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 64, 256)\n",
    "\n",
    "    title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "    title_embedding = Embedding(input_dim=5000, output_dim=100, \n",
    "                                weights=[embedding_matrix], trainable=False)(title_input)\n",
    "    title_gru = GRU(gru_units, return_sequences=True)(title_embedding)\n",
    "    title_lstm = LSTM(lstm_units, return_sequences=False)(title_gru)\n",
    "\n",
    "    description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "    description_embedding = Embedding(input_dim=5000, output_dim=100, \n",
    "                                      weights=[embedding_matrix], trainable=False)(description_input)\n",
    "    description_gru = GRU(gru_units, return_sequences=True)(description_embedding)\n",
    "    description_lstm = LSTM(lstm_units, return_sequences=False)(description_gru)\n",
    "\n",
    "    merged = Concatenate()([title_lstm, description_lstm])\n",
    "    dense_1 = Dense(128, activation='relu')(merged)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)\n",
    "\n",
    "    model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "    model.fit([X_train_title_pad, X_train_description_pad], y_train, \n",
    "              validation_split=0.1, epochs=3, batch_size=16, verbose=0)\n",
    "    loss, accuracy = model.evaluate([X_test_title_pad, X_test_description_pad], y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed39de-0622-4d22-bcb8-7b04a0994845",
   "metadata": {},
   "source": [
    "### Testing module 2 (Optimized Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303543d-d0a9-4b49-a9cd-46738d984fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build optimized model with best parameters\n",
    "best_trial = study.best_trial\n",
    "model_optimized = model_builder(best_trial)\n",
    "\n",
    "# Training and testing optimized model\n",
    "model_optimized.fit([X_train_title_pad, X_train_description_pad], y_train, \n",
    "                    validation_split=0.1, epochs=5, batch_size=16, verbose=1)\n",
    "y_pred_optimized = model_optimized.predict([X_test_title_pad, X_test_description_pad]).argmax(axis=1)\n",
    "\n",
    "# Optimized model metrics\n",
    "print(\"Optimized Model Accuracy:\", np.mean(y_pred_optimized == y_test))\n",
    "print(classification_report(y_test, y_pred_optimized))\n",
    "conf_matrix_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "\n",
    "# Display confusion matrix for optimized model\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_optimized, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Optimized Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
