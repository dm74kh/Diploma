{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8db171-dd8b-49a4-8ca7-4275ecb1cd58",
   "metadata": {
    "id": "de8db171-dd8b-49a4-8ca7-4275ecb1cd58"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8163f6b-47b8-47b7-b9a2-7bbeca1d6fdd",
   "metadata": {
    "id": "a8163f6b-47b8-47b7-b9a2-7bbeca1d6fdd"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a1a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9955a1a5",
    "outputId": "e081ae23-6e1c-47ae-de6b-25ec1e13d66f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {
    "id": "8d3ae439"
   },
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd9176-eff5-451c-94e0-2345ef772618",
   "metadata": {
    "id": "3cfd9176-eff5-451c-94e0-2345ef772618"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_url, header=None)\n",
    "test_df = pd.read_csv(test_url, header=None)\n",
    "train_df.columns = ['Class Index', 'Title', 'Description']\n",
    "test_df.columns = ['Class Index', 'Title', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838ad32-368b-4ed1-9a2b-f5138df4fee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e838ad32-368b-4ed1-9a2b-f5138df4fee3",
    "outputId": "2922875a-6df9-4de1-f058-f853056b1e32"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # convert text to lowercase\n",
    "    text = re.sub(r'\\b(u\\.s\\.|us)\\b', 'usa', text, flags=re.IGNORECASE)  # replace \"U.S.\" or \"US\" with \"usa\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'&\\w+;', '', text)  # remove HTML entities\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove numbers and punctuation (keep only letters and spaces)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] # remove stop-words, tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatization\n",
    "    return ' '.join(words) # combining words into a string\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['clean_text'] = (train_df['Title'] + ' ' + train_df['Description']).apply(preprocess_text)\n",
    "test_df['clean_text'] = (test_df['Title'] + ' ' + test_df['Description']).apply(preprocess_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "max_length_titles = max([len(x.split()) for x in train_df['Title']])\n",
    "max_length_descriptions = max([len(x.split()) for x in train_df['Description']])\n",
    "\n",
    "X_train_title_pad = pad_sequences(tokenizer.texts_to_sequences(train_df['Title']), maxlen=max_length_titles)\n",
    "X_test_title_pad = pad_sequences(tokenizer.texts_to_sequences(test_df['Title']), maxlen=max_length_titles)\n",
    "X_train_description_pad = pad_sequences(tokenizer.texts_to_sequences(train_df['Description']), maxlen=max_length_descriptions)\n",
    "X_test_description_pad = pad_sequences(tokenizer.texts_to_sequences(test_df['Description']), maxlen=max_length_descriptions)\n",
    "\n",
    "y_train = train_df['Class Index'].values - 1\n",
    "y_test = test_df['Class Index'].values - 1\n",
    "\n",
    "# Download and load GloVe embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_index = {}\n",
    "with open('/content/drive/MyDrive/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((5000, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 5000:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {
    "id": "74979b6e"
   },
   "source": [
    "### Basic training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ecc3bd",
    "outputId": "e16503ab-c861-4bbd-bb98-b037b8850ebe"
   },
   "outputs": [],
   "source": [
    "# Define the baseline model with LSTM layers\n",
    "title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "title_embedding = embedding_layer(title_input)\n",
    "title_lstm = LSTM(128, return_sequences=False)(title_embedding)\n",
    "\n",
    "description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "description_embedding = embedding_layer(description_input)\n",
    "description_lstm = LSTM(128, return_sequences=False)(description_embedding)\n",
    "\n",
    "merged = Concatenate()([title_lstm, description_lstm])\n",
    "dense_1 = Dense(128, activation='relu')(merged)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)\n",
    "\n",
    "model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=3e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the baseline model\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, validation_split=0.1,\n",
    "          epochs=5, batch_size=32, callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7938a-c9a9-4c55-a570-a72613fe3ca2",
   "metadata": {
    "id": "2dd7938a-c9a9-4c55-a570-a72613fe3ca2"
   },
   "source": [
    "### Testing Module 1 (Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229b1b2-40af-4099-92e1-6bbdd94e356a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "id": "d229b1b2-40af-4099-92e1-6bbdd94e356a",
    "outputId": "fab0bc2a-4d7e-4cac-b491-99e0b57ea68f"
   },
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "baseline_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Baseline Test Accuracy: {baseline_accuracy:.4f}')\n",
    "print(\"Baseline Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Baseline Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cf88e",
   "metadata": {
    "id": "c75cf88e"
   },
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99bdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09a99bdf",
    "outputId": "29d01f63-0f65-417b-d45e-40b6f8b4f835"
   },
   "outputs": [],
   "source": [
    "def model_builder(trial):\n",
    "    # Define hyperparameters for optimization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 64, 256)\n",
    "    num_units = trial.suggest_int('num_units', 64, 512)\n",
    "\n",
    "    # Define model structure with Optuna parameters\n",
    "    title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "    title_embedding = embedding_layer(title_input)\n",
    "    title_lstm = LSTM(lstm_units, return_sequences=False)(title_embedding)\n",
    "\n",
    "    description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "    description_embedding = embedding_layer(description_input)\n",
    "    description_lstm = LSTM(lstm_units, return_sequences=False)(description_embedding)\n",
    "\n",
    "    merged = Concatenate()([title_lstm, description_lstm])\n",
    "    dense_1 = Dense(num_units, activation='relu')(merged)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)\n",
    "\n",
    "    model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "    model.fit([X_train_title_pad, X_train_description_pad], y_train, validation_split=0.1, epochs=3, batch_size=32, verbose=1)\n",
    "    _, accuracy = model.evaluate([X_test_title_pad, X_test_description_pad], y_test, verbose=1)\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(\"Best Trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0",
   "metadata": {
    "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0"
   },
   "source": [
    "### Testing module 2 (Optimized Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
    "outputId": "8bc49c67-61fb-44b6-dd57-b2f8b36720b8"
   },
   "outputs": [],
   "source": [
    "# Load the best model from Optuna optimization\n",
    "best_trial = study.best_trial\n",
    "model = model_builder(best_trial)\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, validation_split=0.1, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate optimized model\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "final_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "print(\"Final Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Confusion Matrix for optimized model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Optimized Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067f72-a519-4449-ac64-8ae3e97291ec",
   "metadata": {
    "id": "f8067f72-a519-4449-ac64-8ae3e97291ec"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
