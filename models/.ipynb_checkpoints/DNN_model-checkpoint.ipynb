{
 "cells": [
  {
   "cell_type": "raw",
   "id": "86e62499",
   "metadata": {},
   "source": [
    "DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {},
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9955a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea97c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка стоп-слов и лемматизатора\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e78b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # convert text to lowercase\n",
    "    text = re.sub(r'\\b(u\\.s\\.|us)\\b', 'usa', text, flags=re.IGNORECASE)  # replace \"U.S.\" or \"US\" with \"usa\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'&\\w+;', '', text)  # remove HTML entities\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove numbers and punctuation (keep only letters and spaces)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] # remove stop-words, tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatization\n",
    "    return ' '.join(words) # combining words into a string"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25c67259",
   "metadata": {},
   "source": [
    "In addition to the preprocessing performed in EDA (removal of stop words, lemmatization, cleaning of HTML tags, and punctuation), simple tokenization of texts was added. Pretrained GloVe embeddings were used for better word representations, and they were frozen in the Embedding layer, allowing the model to leverage high-quality word vectors without further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4217a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "train_df = pd.read_csv('../data/agn_train.csv')\n",
    "test_df = pd.read_csv('../data/agn_test.csv')\n",
    "\n",
    "# Применение предобработки к заголовкам и описаниям\n",
    "train_df['clean_text'] = (train_df['Title'] + ' ' + train_df['Description']).apply(preprocess_text)\n",
    "test_df['clean_text'] = (test_df['Title'] + ' ' + test_df['Description']).apply(preprocess_text)\n",
    "\n",
    "# Токенизация\n",
    "tokenizer = Tokenizer(num_words=5000)  # Ограничение на количество слов\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])  # Токенизация по тренировочным данным\n",
    "\n",
    "# Токенизация заголовков и описаний\n",
    "X_train_title_seq = tokenizer.texts_to_sequences(train_df['Title'])\n",
    "X_test_title_seq = tokenizer.texts_to_sequences(test_df['Title'])\n",
    "\n",
    "X_train_description_seq = tokenizer.texts_to_sequences(train_df['Description'])\n",
    "X_test_description_seq = tokenizer.texts_to_sequences(test_df['Description'])\n",
    "\n",
    "# Определение максимальной длины\n",
    "max_length_titles = max([len(x) for x in X_train_title_seq])\n",
    "max_length_descriptions = max([len(x) for x in X_train_description_seq])\n",
    "\n",
    "# Выровнять последовательности\n",
    "X_train_title_pad = pad_sequences(X_train_title_seq, maxlen=max_length_titles)\n",
    "X_test_title_pad = pad_sequences(X_test_title_seq, maxlen=max_length_titles)\n",
    "\n",
    "X_train_description_pad = pad_sequences(X_train_description_seq, maxlen=max_length_descriptions)\n",
    "X_test_description_pad = pad_sequences(X_test_description_seq, maxlen=max_length_descriptions)\n",
    "\n",
    "# Определение меток для тренировочных данных\n",
    "y_train = train_df['Class Index'].values - 1  # Приводим классы к диапазону [0, 1, 2, 3]\n",
    "\n",
    "# Определение меток для тестовых данных\n",
    "y_test = test_df['Class Index'].values - 1  # Приводим классы к диапазону [0, 1, 2, 3]\n",
    "\n",
    "# Загрузка предобученных эмбеддингов GloVe\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Создание матрицы эмбеддингов для слов в токенизаторе\n",
    "embedding_matrix = np.zeros((5000, 100))  # input_dim=5000, output_dim=100 (размерность векторов GloVe)\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 5000:  # Только слова из топ-5000\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Использование предварительно обученных эмбеддингов (GloVe)\n",
    "embedding_layer = Embedding(input_dim=5000, \n",
    "                            output_dim=100, \n",
    "                            weights=[embedding_matrix],  # Передача предобученной матрицы эмбеддингов\n",
    "                            input_length=max_length_titles, \n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {},
   "source": [
    "### Training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbd1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3750/3750 [==============================] - 23s 6ms/step - loss: 0.3641 - accuracy: 0.8713 - val_loss: 0.3653 - val_accuracy: 0.8718\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 21s 5ms/step - loss: 0.2841 - accuracy: 0.8994 - val_loss: 0.3397 - val_accuracy: 0.8855\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.2263 - accuracy: 0.9193 - val_loss: 0.3521 - val_accuracy: 0.8862\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 21s 5ms/step - loss: 0.1708 - accuracy: 0.9392 - val_loss: 0.4026 - val_accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.1254 - accuracy: 0.9551 - val_loss: 0.4526 - val_accuracy: 0.8817\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 0.0927 - accuracy: 0.9672 - val_loss: 0.5532 - val_accuracy: 0.8729\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0703 - accuracy: 0.9751 - val_loss: 0.6531 - val_accuracy: 0.8787\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 22s 6ms/step - loss: 0.0589 - accuracy: 0.9798 - val_loss: 0.7609 - val_accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 0.0492 - accuracy: 0.9832 - val_loss: 0.7994 - val_accuracy: 0.8687\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 0.0435 - accuracy: 0.9854 - val_loss: 0.8897 - val_accuracy: 0.8675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ca29117750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вход для заголовков\n",
    "title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "title_embedding = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], input_length=max_length_titles, trainable=False)(title_input)\n",
    "title_flatten = Flatten()(title_embedding)\n",
    "\n",
    "# Вход для описаний\n",
    "description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "description_embedding = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], input_length=max_length_descriptions, trainable=False)(description_input)\n",
    "description_flatten = Flatten()(description_embedding)\n",
    "\n",
    "# Объединение представлений заголовков и описаний\n",
    "merged = Concatenate()([title_flatten, description_flatten])\n",
    "\n",
    "# Полносвязные слои после объединения\n",
    "dense_1 = Dense(128, activation='relu')(merged)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)  # 4 класса\n",
    "\n",
    "# Создание модели\n",
    "model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Обучение модели с раздельной обработкой заголовков и описаний\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, epochs=10, batch_size=32, validation_data=([X_test_title_pad, X_test_description_pad], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68022d89",
   "metadata": {},
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b3f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
