{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93200413-eef4-474e-819a-00fc72722fc8",
   "metadata": {
    "id": "93200413-eef4-474e-819a-00fc72722fc8"
   },
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08be2a8-bce8-4f46-ae6f-489e050f682a",
   "metadata": {
    "id": "c08be2a8-bce8-4f46-ae6f-489e050f682a"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a1a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9955a1a5",
    "outputId": "1f4697de-d97d-4478-d2ba-674a5c63ef71"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {
    "id": "8d3ae439"
   },
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35308feb-8d17-4d19-aacf-db36835cbeff",
   "metadata": {
    "id": "35308feb-8d17-4d19-aacf-db36835cbeff"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_url, header=None)\n",
    "test_df = pd.read_csv(test_url, header=None)\n",
    "train_df.columns = ['Class Index', 'Title', 'Description']\n",
    "test_df.columns = ['Class Index', 'Title', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d133fe-d776-4195-9304-3d8b7f7b5fd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01d133fe-d776-4195-9304-3d8b7f7b5fd9",
    "outputId": "2a8b65ef-535c-488b-e974-cfa672126a4a"
   },
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # convert text to lowercase\n",
    "    text = re.sub(r'\\b(u\\.s\\.|us)\\b', 'usa', text, flags=re.IGNORECASE)  # replace \"U.S.\" or \"US\" with \"usa\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'&\\w+;', '', text)  # remove HTML entities\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove numbers and punctuation (keep only letters and spaces)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] # remove stop-words, tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatization\n",
    "    return ' '.join(words) # combining words into a string\n",
    "\n",
    "# Applying preprocessing\n",
    "train_df['clean_text'] = (train_df['Title'] + ' ' + train_df['Description']).apply(preprocess_text)\n",
    "test_df['clean_text'] = (test_df['Title'] + ' ' + test_df['Description']).apply(preprocess_text)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "# Prepare padded sequences for titles and descriptions\n",
    "max_length_titles = max([len(x.split()) for x in train_df['Title']])\n",
    "max_length_descriptions = max([len(x.split()) for x in train_df['Description']])\n",
    "\n",
    "X_train_title_pad = pad_sequences(tokenizer.texts_to_sequences(train_df['Title']), maxlen=max_length_titles)\n",
    "X_test_title_pad = pad_sequences(tokenizer.texts_to_sequences(test_df['Title']), maxlen=max_length_titles)\n",
    "\n",
    "X_train_description_pad = pad_sequences(tokenizer.texts_to_sequences(train_df['Description']), maxlen=max_length_descriptions)\n",
    "X_test_description_pad = pad_sequences(tokenizer.texts_to_sequences(test_df['Description']), maxlen=max_length_descriptions)\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['Class Index'].values - 1  # Adjust indices to [0, 1, 2, 3]\n",
    "y_test = test_df['Class Index'].values - 1\n",
    "\n",
    "# Download and load GloVe embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_index = {}\n",
    "with open('/content/drive/MyDrive/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((5000, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 5000:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {
    "id": "74979b6e"
   },
   "source": [
    "### Bacic training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55ecc3bd",
    "outputId": "d010b01e-407e-4039-d851-1fd17b9ca5fd"
   },
   "outputs": [],
   "source": [
    "# Build baseline model\n",
    "title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "title_embedding = embedding_layer(title_input)\n",
    "title_flatten = Flatten()(title_embedding)\n",
    "\n",
    "description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "description_embedding = embedding_layer(description_input)\n",
    "description_flatten = Flatten()(description_embedding)\n",
    "\n",
    "merged = Concatenate()([title_flatten, description_flatten])\n",
    "\n",
    "dense_1 = Dense(128, activation='relu')(merged)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)\n",
    "\n",
    "model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=3e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train baseline model\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, validation_split=0.1,\n",
    "          epochs=5, batch_size=32, callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc17b8e-ffa6-4a3f-a410-062a8f15e3e6",
   "metadata": {
    "id": "4dc17b8e-ffa6-4a3f-a410-062a8f15e3e6"
   },
   "source": [
    "### Testing Module 1 (Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d3165-d74f-427c-aed7-fbc5255f7570",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 814
    },
    "id": "373d3165-d74f-427c-aed7-fbc5255f7570",
    "outputId": "ad85ffd5-2782-419c-d38f-5b13d21f0260"
   },
   "outputs": [],
   "source": [
    "# Test the baseline model\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "baseline_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Baseline Test Accuracy: {baseline_accuracy:.4f}')\n",
    "print(\"Baseline Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Baseline Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cf88e",
   "metadata": {
    "id": "c75cf88e"
   },
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99bdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09a99bdf",
    "outputId": "58a39691-ec5b-4686-de27-d94973961dab"
   },
   "outputs": [],
   "source": [
    "def model_builder(trial):\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    num_units = trial.suggest_int('num_units', 64, 512)\n",
    "\n",
    "    title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "    title_embedding = embedding_layer(title_input)\n",
    "    title_flatten = Flatten()(title_embedding)\n",
    "\n",
    "    description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "    description_embedding = embedding_layer(description_input)\n",
    "    description_flatten = Flatten()(description_embedding)\n",
    "\n",
    "    merged = Concatenate()([title_flatten, description_flatten])\n",
    "    dense_1 = Dense(num_units, activation='relu')(merged)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)\n",
    "\n",
    "    model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "    model.fit([X_train_title_pad, X_train_description_pad], y_train, validation_split=0.1, epochs=3, batch_size=32, verbose=1)\n",
    "    _, accuracy = model.evaluate([X_test_title_pad, X_test_description_pad], y_test, verbose=1)\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(\"Best Trial:\", study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0",
   "metadata": {
    "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0"
   },
   "source": [
    "### Testing module 2 (Optimized Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
    "outputId": "8ce51b34-67fb-4884-9373-bb8cf0251e07"
   },
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "model = model_builder(best_trial)\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train, validation_split=0.1, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "final_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Final Test Accuracy: {final_accuracy:.4f}')\n",
    "print(\"Final Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Final Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067f72-a519-4449-ac64-8ae3e97291ec",
   "metadata": {
    "id": "f8067f72-a519-4449-ac64-8ae3e97291ec"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
