{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8db171-dd8b-49a4-8ca7-4275ecb1cd58",
   "metadata": {},
   "source": [
    "## CNN-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6915b95-c61b-492e-8a01-0dcd537a7d81",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dropout, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {},
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c746bb4-6c7c-467a-b28b-7ee46514e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_url, header=None)\n",
    "test_df = pd.read_csv(test_url, header=None)\n",
    "\n",
    "train_df.columns = ['Class Index', 'Title', 'Description']\n",
    "test_df.columns = ['Class Index', 'Title', 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5d541-f692-4569-a6b2-7453b200ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop_words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # convert text to lowercase\n",
    "    text = re.sub(r'\\b(u\\.s\\.|us)\\b', 'usa', text, flags=re.IGNORECASE)  # replace \"U.S.\" or \"US\" with \"usa\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'&\\w+;', '', text)  # remove HTML entities\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove numbers and punctuation (keep only letters and spaces)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] # remove stop-words, tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatization\n",
    "    return ' '.join(words) # combining words into a string\n",
    "\n",
    "# Preprocess text data\n",
    "train_df['clean_text'] = (train_df['Title'] + ' ' + train_df['Description']).apply(preprocess_text)\n",
    "test_df['clean_text'] = (test_df['Title'] + ' ' + test_df['Description']).apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000) # The tokenizer will only consider the top 5000 words in the training data to limit vocabulary size for embedding.\n",
    "tokenizer.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "# Convert the 'Title' and 'Description' columns into sequences of integer indices\n",
    "X_train_title_seq = tokenizer.texts_to_sequences(train_df['Title'])\n",
    "X_test_title_seq = tokenizer.texts_to_sequences(test_df['Title'])\n",
    "X_train_description_seq = tokenizer.texts_to_sequences(train_df['Description'])\n",
    "X_test_description_seq = tokenizer.texts_to_sequences(test_df['Description'])\n",
    "\n",
    "# Padding sequences\n",
    "max_length_titles = max(len(x) for x in X_train_title_seq)  # Max length of title sequences\n",
    "max_length_descriptions = max(len(x) for x in X_train_description_seq)  # Max length of description sequences\n",
    "X_train_title_pad = pad_sequences(X_train_title_seq, maxlen=max_length_titles)  # Pad title sequences in training data\n",
    "X_test_title_pad = pad_sequences(X_test_title_seq, maxlen=max_length_titles)  # Pad title sequences in test data\n",
    "X_train_description_pad = pad_sequences(X_train_description_seq, maxlen=max_length_descriptions)  # Pad description sequences in training data\n",
    "X_test_description_pad = pad_sequences(X_test_description_seq, maxlen=max_length_descriptions)  # Pad description sequences in test data\n",
    "\n",
    "# Assign target labels\n",
    "y_train = train_df['Class Index'].values - 1  # Class indices in training data, adjusted to start at 0\n",
    "y_test = test_df['Class Index'].values - 1  # Class indices in test data, adjusted to start at 0\n",
    "\n",
    "\n",
    "### Load GloVe Embeddings\n",
    "\n",
    "# Download GloVe embeddings\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip\n",
    "\n",
    "# Creating an index of GloVe embeddings\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()  # Split the line into words\n",
    "        word = values[0]  # The first value is the word itself\n",
    "        coefs = np.asarray(values[1:], dtype='float32')  # The rest are the embedding coefficients\n",
    "        embedding_index[word] = coefs  # Store the word and its embedding vector in a dictionary\n",
    "\n",
    "# Building an embedding matrix for the tokens from our dataset\n",
    "embedding_matrix = np.zeros((5000, 100))  # Initialize with zeros; size: 5000 words, 100-dimensional embeddings\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 5000:  # Consider only the top 5000 words\n",
    "        embedding_vector = embedding_index.get(word)  # Retrieve the embedding vector if it exists\n",
    "        if embedding_vector is not None:  # If an embedding was found\n",
    "            embedding_matrix[i] = embedding_vector  # Fill the embedding matrix\n",
    "\n",
    "# Creating the Embedding layer using GloVe embeddings\n",
    "embedding_layer = Embedding(input_dim=5000, output_dim=100, weights=[embedding_matrix], trainable=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {},
   "source": [
    "### Basic training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "title_input = Input(shape=(max_length_titles,), name='title_input')\n",
    "title_embedding = embedding_layer(title_input)\n",
    "title_conv = Conv1D(filters=128, kernel_size=5, activation='relu')(title_embedding)\n",
    "title_pooling = MaxPooling1D(pool_size=2)(title_conv)\n",
    "title_lstm = LSTM(128, return_sequences=False)(title_pooling)\n",
    "\n",
    "description_input = Input(shape=(max_length_descriptions,), name='description_input')\n",
    "description_embedding = embedding_layer(description_input)\n",
    "description_conv = Conv1D(filters=128, kernel_size=5, activation='relu')(description_embedding)\n",
    "description_pooling = MaxPooling1D(pool_size=2)(description_conv)\n",
    "description_lstm = LSTM(128, return_sequences=False)(description_pooling)\n",
    "\n",
    "merged = Concatenate()([title_lstm, description_lstm])\n",
    "dense_1 = Dense(128, activation='relu')(merged)\n",
    "dense_2 = Dense(64, activation='relu')(dense_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)\n",
    "\n",
    "model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train,\n",
    "          validation_split=0.1, epochs=5, batch_size=32, verbose=1,\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4c866-d557-488f-9fe3-6a11250100c2",
   "metadata": {},
   "source": [
    "### Testing module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915a440-3b45-4748-8a6e-ea8c6f2eb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on base model\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "test_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Base Test Accuracy: {test_accuracy:.4f}')\n",
    "print(\"Classification Report (Base Model):\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Base Model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cf88e",
   "metadata": {},
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(trial):\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 64, 256)\n",
    "    cnn_filters = trial.suggest_int('cnn_filters', 64, 256)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 5)\n",
    "    pool_size = trial.suggest_int('pool_size', 2, 3)\n",
    "    num_units = trial.suggest_int('num_units', 64, 512)\n",
    "\n",
    "    title_input = Input(shape=(max_length_titles,))\n",
    "    title_embedding = embedding_layer(title_input)\n",
    "    title_conv = Conv1D(filters=cnn_filters, kernel_size=kernel_size, activation='relu')(title_embedding)\n",
    "    title_pooling = MaxPooling1D(pool_size=pool_size)(title_conv)\n",
    "    title_lstm = LSTM(lstm_units, return_sequences=False)(title_pooling)\n",
    "\n",
    "    description_input = Input(shape=(max_length_descriptions,))\n",
    "    description_embedding = embedding_layer(description_input)\n",
    "    description_conv = Conv1D(filters=cnn_filters, kernel_size=kernel_size, activation='relu')(description_embedding)\n",
    "    description_pooling = MaxPooling1D(pool_size=pool_size)(description_conv)\n",
    "    description_lstm = LSTM(lstm_units, return_sequences=False)(description_pooling)\n",
    "\n",
    "    merged = Concatenate()([title_lstm, description_lstm])\n",
    "    dense_1 = Dense(num_units, activation='relu')(merged)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)\n",
    "\n",
    "    model = Model(inputs=[title_input, description_input], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "    model.fit([X_train_title_pad, X_train_description_pad], y_train,\n",
    "              validation_split=0.1, epochs=3, batch_size=32, verbose=1)\n",
    "    loss, accuracy = model.evaluate([X_test_title_pad, X_test_description_pad], y_test, verbose=1)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0",
   "metadata": {},
   "source": [
    "### Testing module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "model = model_builder(best_trial)\n",
    "model.fit([X_train_title_pad, X_train_description_pad], y_train,\n",
    "          validation_split=0.1, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = model.predict([X_test_title_pad, X_test_description_pad])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "test_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Optimized Test Accuracy: {test_accuracy:.4f}')\n",
    "print(\"Classification Report (Optimized Model):\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Optimized Model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067f72-a519-4449-ac64-8ae3e97291ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
