{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8db171-dd8b-49a4-8ca7-4275ecb1cd58",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde59a40-6263-4fd8-a0af-612130ed597a",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Layer, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {},
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0e7a8-78c3-494d-b59c-00a5519b61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_url, header=None)\n",
    "test_df = pd.read_csv(test_url, header=None)\n",
    "\n",
    "train_df.columns = ['Class Index', 'Title', 'Description']\n",
    "test_df.columns = ['Class Index', 'Title', 'Description']\n",
    "train_df['clean_text'] = train_df['Title'] + ' ' + train_df['Description']\n",
    "test_df['clean_text'] = test_df['Title'] + ' ' + test_df['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9476a-b233-4ccf-b11e-b4b5844a0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer and define tokenization function\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def preprocess_for_bert(texts, max_len):\n",
    "    \"\"\"Tokenize text for BERT\"\"\"\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "\n",
    "# Set maximum sequence length and preprocess text data\n",
    "max_len = 128\n",
    "X_train_bert = preprocess_for_bert(train_df['clean_text'], max_len)\n",
    "X_test_bert = preprocess_for_bert(test_df['clean_text'], max_len)\n",
    "y_train = train_df['Class Index'].values - 1\n",
    "y_test = test_df['Class Index'].values - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {},
   "source": [
    "### Basic training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom layer for BERT embedding\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output\n",
    "\n",
    "# Define model structure with custom BERT layer\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks = Input(shape=(max_len,), dtype=tf.int32, name='attention_masks')\n",
    "bert_output = BertLayer()([input_ids, attention_masks])\n",
    "\n",
    "# Add dense layers for classification\n",
    "dense_1 = Dense(128, activation='relu')(bert_output)\n",
    "dropout_1 = Dropout(0.3)(dense_1)\n",
    "output = Dense(4, activation='softmax')(dropout_1)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the base model\n",
    "model.fit(\n",
    "    [X_train_bert['input_ids'], X_train_bert['attention_mask']], y_train,\n",
    "    validation_split=0.1, epochs=5, batch_size=32, verbose=1,\n",
    "    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ffb9b-7109-456e-8df6-1ccd7fa7a132",
   "metadata": {},
   "source": [
    "### Testing module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289c7e7-59ea-4681-9386-7703b0a9d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the base model\n",
    "y_pred = model.predict([X_test_bert['input_ids'], X_test_bert['attention_mask']])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "test_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Base Test Accuracy: {test_accuracy:.4f}')\n",
    "print(\"Classification Report (Base Model):\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Base Model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cf88e",
   "metadata": {},
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model for Optuna hyperparameter tuning\n",
    "def model_builder(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
    "    num_units = trial.suggest_int('num_units', 64, 512)\n",
    "\n",
    "    # Model structure with hyperparameter-optimized dense layers\n",
    "    input_ids = Input(shape=(max_len,), dtype='int32', name='input_ids')\n",
    "    attention_masks = Input(shape=(max_len,), dtype='int32', name='attention_masks')\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def bert_layer(inputs):\n",
    "        return bert_model(inputs[0], attention_mask=inputs[1]).pooler_output\n",
    "\n",
    "    bert_output = Lambda(bert_layer, output_shape=(768,))([input_ids, attention_masks])\n",
    "    dense_1 = Dense(num_units, activation='relu')(bert_output)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)\n",
    "\n",
    "    # Compile the model with optimized learning rate\n",
    "    model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "    model.fit([X_train_bert['input_ids'], X_train_bert['attention_mask']], y_train,\n",
    "              validation_split=0.1, epochs=3, batch_size=32, verbose=1)\n",
    "    loss, accuracy = model.evaluate([X_test_bert['input_ids'], X_test_bert['attention_mask']], y_test, verbose=1)\n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0",
   "metadata": {},
   "source": [
    "### Testing module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and build best model from Optuna study\n",
    "best_trial = study.best_trial\n",
    "model = model_builder(best_trial)\n",
    "\n",
    "# Train model with best hyperparameters\n",
    "model.fit([X_train_bert['input_ids'], X_train_bert['attention_mask']], y_train,\n",
    "          validation_split=0.1, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Test the optimized model\n",
    "y_pred = model.predict([X_test_bert['input_ids'], X_test_bert['attention_mask']])\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "test_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Optimized Test Accuracy: {test_accuracy:.4f}')\n",
    "print(\"Classification Report (Optimized Model):\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Display confusion matrix for optimized model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (Optimized Model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067f72-a519-4449-ac64-8ae3e97291ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
