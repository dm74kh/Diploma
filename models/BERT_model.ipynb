{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8db171-dd8b-49a4-8ca7-4275ecb1cd58",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae439",
   "metadata": {},
   "source": [
    "### Preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f875b9-c44e-45c2-9413-75e7e56a7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "train_df = pd.read_csv('../data/agn_train.csv')\n",
    "test_df = pd.read_csv('../data/agn_test.csv')\n",
    "\n",
    "# Loading the data (same as in CNN-RNN model)\n",
    "# train_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv'\n",
    "# test_url = 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'\n",
    "\n",
    "# train_df = pd.read_csv(train_url, header=None)\n",
    "# test_df = pd.read_csv(test_url, header=None)\n",
    "\n",
    "# # Add column names\n",
    "# train_df.columns = ['Class Index', 'Title', 'Description']\n",
    "# test_df.columns = ['Class Index', 'Title', 'Description']\n",
    "\n",
    "# Combine 'Title' and 'Description' into 'clean_text'\n",
    "train_df['clean_text'] = train_df['Title'] + ' ' + train_df['Description']\n",
    "test_df['clean_text'] = test_df['Title'] + ' ' + test_df['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd19004-13de-4a1a-94a5-6c97f6b5f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function for preprocessing data using the BERT tokenizer\n",
    "def preprocess_for_bert(texts, max_len):\n",
    "    \"\"\"Tokenize and convert text to BERT's input format.\"\"\"\n",
    "    return tokenizer(\n",
    "        texts.tolist(),                # Convert DataFrame to list\n",
    "        add_special_tokens=True,       # Add [CLS] and [SEP]\n",
    "        max_length=max_len,            # Max length of the sequence\n",
    "        truncation=True,               # Truncate if longer than max_len\n",
    "        padding='max_length',          # Pad to max length\n",
    "        return_attention_mask=True,    # Create attention mask\n",
    "        return_tensors='np'            # Return numpy arrays for tensors\n",
    "    )\n",
    "\n",
    "# Define the maximum sequence length for BERT\n",
    "max_len = 128  # BERT typically works well with lengths of 128 or 256 tokens\n",
    "\n",
    "# Tokenize and preprocess training and test data for BERT\n",
    "X_train_bert = preprocess_for_bert(train_df['clean_text'], max_len)\n",
    "X_test_bert = preprocess_for_bert(test_df['clean_text'], max_len)\n",
    "\n",
    "# Prepare labels\n",
    "y_train = train_df['Class Index'].values - 1  # Adjust class indices to [0, 1, 2, 3]\n",
    "y_test = test_df['Class Index'].values - 1  # Adjust class indices to [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979b6e",
   "metadata": {},
   "source": [
    "### Training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Keras layer to wrap BERT model\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert = bert_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return output.pooler_output\n",
    "\n",
    "# Define inputs for BERT (input_ids, attention_masks)\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks = Input(shape=(max_len,), dtype=tf.int32, name='attention_masks')\n",
    "\n",
    "# Use the custom BertLayer\n",
    "bert_output = BertLayer()([input_ids, attention_masks])\n",
    "\n",
    "# Add a fully connected layer\n",
    "dense_1 = Dense(128, activation='relu')(bert_output)\n",
    "dropout_1 = Dropout(0.3)(dense_1)\n",
    "dense_2 = Dense(64, activation='relu')(dropout_1)\n",
    "output = Dense(4, activation='softmax')(dense_2)  # 4 classes\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=3e-5)  # Smaller learning rate for fine-tuning BERT\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training parameters\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    [X_train_bert['input_ids'], X_train_bert['attention_mask']], \n",
    "    y_train, \n",
    "    validation_split=0.1, \n",
    "    epochs=10,  # Fine-tuning usually requires fewer epochs\n",
    "    batch_size=16,  # Smaller batch sizes are often used for BERT\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cf88e",
   "metadata": {},
   "source": [
    "### Hyperparameter selection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a99bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining hyperparameters with Optuna\n",
    "def model_builder(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)  # Smaller range for BERT fine-tuning\n",
    "    num_units = trial.suggest_int('num_units', 64, 512)  # For dense layers\n",
    "\n",
    "    # Load the pre-trained BERT model\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Input for BERT (input_ids, attention_masks)\n",
    "    input_ids = Input(shape=(max_len,), dtype='int32', name='input_ids')\n",
    "    attention_masks = Input(shape=(max_len,), dtype='int32', name='attention_masks')\n",
    "\n",
    "    # BERT output (pooled_output)\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_masks)\n",
    "    pooled_output = bert_output.pooler_output\n",
    "\n",
    "    # Fully connected layers after BERT\n",
    "    dense_1 = Dense(num_units, activation='relu')(pooled_output)\n",
    "    dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "    output = Dense(4, activation='softmax')(dropout_1)  # 4 classes\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for model evaluation\n",
    "def objective(trial):\n",
    "    model = model_builder(trial)\n",
    "\n",
    "    # Train the model with the optimized hyperparameters\n",
    "    model.fit(\n",
    "        [X_train_bert['input_ids'], X_train_bert['attention_mask']], \n",
    "        y_train, \n",
    "        validation_split=0.1, \n",
    "        epochs=3,  # Fine-tuning BERT usually requires fewer epochs\n",
    "        batch_size=16,  # Smaller batch sizes are often used for BERT\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate the model's accuracy\n",
    "    loss, accuracy = model.evaluate([X_test_bert['input_ids'], X_test_bert['attention_mask']], y_test, verbose=1)\n",
    "    return accuracy  # Optimizing for accuracy\n",
    "\n",
    "# Running Optuna to search for the best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')  # Maximize accuracy\n",
    "study.optimize(objective, n_trials=10)  # Number of optimization trials\n",
    "\n",
    "# Print the best trial results\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05cc49-7725-4cc1-aea4-ed059e188af0",
   "metadata": {},
   "source": [
    "### Testing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46cd1e-6845-4f0e-835c-ce96554e3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the best model using the best hyperparameters found by Optuna\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "model = model_builder(best_trial)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit([X_train_bert['input_ids'], X_train_bert['attention_mask']], y_train, \n",
    "          validation_split=0.1, epochs=3, batch_size=16, verbose=0)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict([X_test_bert['input_ids'], X_test_bert['attention_mask']])\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert predicted probabilities to predicted classes\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = (y_pred_classes == y_test).mean()\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Generate classification report (Precision, Recall, F1-Score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8067f72-a519-4449-ac64-8ae3e97291ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
